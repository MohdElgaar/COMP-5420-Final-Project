{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformer_mt.modeling_transformer import TransfomerEncoderDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You probably need to modify the paths to your tokenizers\n",
    "# For tokenizers, you need to provide the path to a directory that contains the tokenizer.json file\n",
    "# For model, you need to provide the path to a directory that contains the model.pt file\n",
    "\n",
    "source_tokenizer = PreTrainedTokenizerFast.from_pretrained(\"../en_de_output_dir/en_tokenizer\")\n",
    "target_tokenizer = PreTrainedTokenizerFast.from_pretrained(\"../en_de_output_dir/de_tokenizer\")\n",
    "\n",
    "model = TransfomerEncoderDecoderModel.from_pretrained(\"../en_de_output_dir\")\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Try out your model. Feel free to use your own sentences and to compare the model outputs to Google Translate.\n",
    "Find at least three sentences that are translated well, and one that is translated badly.\n",
    "\n",
    "Feel free to change beam size and max_length parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es ist ein guter Tag.\n"
     ]
    }
   ],
   "source": [
    "input_ids = source_tokenizer.encode(\"It's a good day.\", return_tensors=\"pt\")\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=10,\n",
    "    bos_token_id=target_tokenizer.bos_token_id,\n",
    "    eos_token_id=target_tokenizer.eos_token_id,\n",
    "    pad_token_id=target_tokenizer.pad_token_id,\n",
    ")\n",
    "print(target_tokenizer.decode(output_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warum haben Sie das getan?\n"
     ]
    }
   ],
   "source": [
    "input_ids = source_tokenizer.encode(\"Why did you do this?\", return_tensors=\"pt\")\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=10,\n",
    "    bos_token_id=target_tokenizer.bos_token_id,\n",
    "    eos_token_id=target_tokenizer.eos_token_id,\n",
    "    pad_token_id=target_tokenizer.pad_token_id,\n",
    ")\n",
    "print(target_tokenizer.decode(output_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary spielt das Klavier\n"
     ]
    }
   ],
   "source": [
    "input_ids = source_tokenizer.encode(\"Mary plays the piano.\", return_tensors=\"pt\")\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=4,\n",
    "    bos_token_id=target_tokenizer.bos_token_id,\n",
    "    eos_token_id=target_tokenizer.eos_token_id,\n",
    "    pad_token_id=target_tokenizer.pad_token_id,\n",
    ")\n",
    "print(target_tokenizer.decode(output_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ich werde Sie wissen, dass ich 端ber f端nfzig Rom ane geschrieben habe, die 端ber f端nfzig Rom ane geschrieben\n"
     ]
    }
   ],
   "source": [
    "input_ids = source_tokenizer.encode(\"I'll have you know I've written over fifty novels\", return_tensors=\"pt\")\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=20,\n",
    "    bos_token_id=target_tokenizer.bos_token_id,\n",
    "    eos_token_id=target_tokenizer.eos_token_id,\n",
    "    pad_token_id=target_tokenizer.pad_token_id,\n",
    ")\n",
    "print(target_tokenizer.decode(output_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Buch steht vor dem Tisch.\n"
     ]
    }
   ],
   "source": [
    "input_ids = source_tokenizer.encode(\"The book is in front of the table.\", return_tensors=\"pt\")\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=10,\n",
    "    bos_token_id=target_tokenizer.bos_token_id,\n",
    "    eos_token_id=target_tokenizer.eos_token_id,\n",
    "    pad_token_id=target_tokenizer.pad_token_id,\n",
    ")\n",
    "print(target_tokenizer.decode(output_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der Geheim code, den sie geschaffen haben, machte keinen Sinn, auch sie.\n"
     ]
    }
   ],
   "source": [
    "input_ids = source_tokenizer.encode(\"The secret code they created made no sense, even to them.\", return_tensors=\"pt\")\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    bos_token_id=target_tokenizer.bos_token_id,\n",
    "    eos_token_id=target_tokenizer.eos_token_id,\n",
    "    pad_token_id=target_tokenizer.pad_token_id,\n",
    ")\n",
    "print(target_tokenizer.decode(output_ids[0]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f86d5ac646110a331c60478f9400a1a64d0cd523be90d887457228f9723636b5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
